import os
import time
import random
import subprocess
import gc
import soundfile as sf
import numpy as np
import threading
import queue
import asyncio
import requests
import torch
from diffusers import StableDiffusionPipeline
import edge_tts
from crewai import Agent, Task, Crew

# === –ù–û–í–´–ï –ò–ú–ü–û–†–¢–´ –î–õ–Ø STABLE AUDIO ===
from huggingface_hub import login
from stable_audio_tools import get_pretrained_model
from stable_audio_tools.inference import generate_diffusion_cond

# =========================
# 1. CONFIG & ENV
# =========================
os.environ["HF_HOME"] = "/workspace/hf_cache"

# –ö–õ–Æ–ß–ò
STREAM_KEY = os.environ.get("TWITCH_STREAM_KEY")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
HF_TOKEN = os.environ.get("HF_TOKEN") # <--- –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ù–£–ñ–ï–ù!

if not STREAM_KEY:
    print("‚ö†Ô∏è WARNING: TWITCH_STREAM_KEY not found.")
if not HF_TOKEN:
    print("‚ùå CRITICAL: HF_TOKEN not found! Stable Audio won't download.")
    # –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω–∞ –Ω–µ—Ç –≤ ENV, –ø–æ–ø—Ä–æ–±—É–µ–º –≤–æ–π—Ç–∏ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ –∏–ª–∏ —É–ø–∞–¥–µ–º
else:
    print("üîë Logging into HuggingFace...")
    login(token=HF_TOKEN)

RTMP_URL = f"rtmp://live.twitch.tv/app/{STREAM_KEY}"
WORKDIR = "/workspace/airadio/data"
os.makedirs(WORKDIR, exist_ok=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"‚öôÔ∏è Device: {DEVICE}")

# –û—á–µ—Ä–µ–¥—å —Å–µ–≥–º–µ–Ω—Ç–æ–≤
video_queue = queue.Queue(maxsize=3) # –£–º–µ–Ω—å—à–∏–ª –±—É—Ñ–µ—Ä –¥–æ 3, —Ç.–∫. –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–æ–ª—å—à–µ
TRACKS_BEFORE_DJ = 3 

# =========================
# 2. LOAD MODELS
# =========================
def cleanup():
    gc.collect()
    torch.cuda.empty_cache()

print("‚è≥ Loading Stable Audio Open 1.0...")
# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Stable Audio
# –û–Ω–∞ —Ç—è–∂–µ–ª–∞—è, –ø–æ—ç—Ç–æ–º—É —Å—Ä–∞–∑—É —á–∏—Å—Ç–∏–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
cleanup()
audio_model, audio_cfg = get_pretrained_model("stabilityai/stable-audio-open-1.0", device=DEVICE)
audio_model.to(DEVICE).eval()

print("‚è≥ Loading Stable Diffusion...")
sd_pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True
).to(DEVICE)
sd_pipe.safety_checker = None

# =========================
# 3. CREW AI & LOGIC
# =========================
TECH_KEYWORDS = ["AI", "ML", "OpenAI", "LLM", "NVIDIA", "Robotics", "SpaceX", "Python", "Cyberpunk", "Neural"]

def fetch_tech_news():
    try:
        r = requests.get(
            "https://hn.algolia.com/api/v1/search_by_date",
            params={"query": "AI", "tags": "story", "hitsPerPage": 15},
            timeout=10
        )
        hits = r.json().get("hits", [])
        news = [h["title"] for h in hits if any(k.lower() in h.get("title", "").lower() for k in TECH_KEYWORDS)]
        return news[:3] if news else ["Neural networks are evolving rapidly."]
    except Exception as e:
        print(f"‚ö†Ô∏è News fetch failed: {e}")
        return ["Systems operating normally."]

class CrewAIDJ:
    def __init__(self):
        if not OPENAI_API_KEY:
            self.agent = None
            return

        self.agent = Agent(
            role="Cyberpunk Radio Host",
            goal="Deliver short, cool updates about technology between music tracks.",
            backstory="You are 'Nexus', an AI host on a futuristic radio station playing Punk and Electronic Rock.",
            verbose=False,
            allow_delegation=False
        )

    def generate_script(self, mood="high energy"):
        if not self.agent:
            return "System status nominal. Crank up the volume."

        news_items = fetch_tech_news()
        news_str = "\n- ".join(news_items)

        task = Task(
            description=f"""
            Live on air. Mood: {mood}.
            Tech Headlines: {news_str}
            Instructions:
            1. Mention one headline briefly.
            2. Be cool, concise, energetic.
            3. Under 3 sentences.
            """,
            agent=self.agent,
            expected_output="Short DJ script."
        )

        crew = Crew(agents=[self.agent], tasks=[task])
        try:
            return str(crew.kickoff())
        except Exception as e:
            print(f"‚ö†Ô∏è CrewAI Error: {e}")
            return "Data stream synchronized. Listen to this."

ai_dj = CrewAIDJ()

# === –ù–û–í–´–ï –ü–†–û–ú–ü–¢–´ –î–õ–Ø STABLE AUDIO (–õ–Æ–ë–ò–¢ –°–ü–ò–°–ö–ò –ß–ï–†–ï–ó –ó–ê–ü–Ø–¢–£–Æ) ===
def get_vibes():
    genres = [
        (
            "punk rock, fast tempo, distorted guitars, aggressive drums, high fidelity, studio recording, heavy bass", 
            "punk rock poster, anarchy symbol, graffiti, red and black, grunge texture"
        ),
        (
            "post-punk, dark wave, chorus guitar, driving bassline, melancholic, 80s goth vibe, reverb, atmospheric", 
            "post-punk album cover, monochrome, brutalist architecture, dark fog"
        ),
        (
            "happy hardcore, 170bpm, energetic piano, heavy kick drum, rave, dance, synthesizer, uplifting", 
            "colorful rave party, lasers, neon rainbows, high energy"
        ),
        (
            "electronic rock, industrial metal, distorted synths, powerful drums, cyberpunk action, cinematic", 
            "cyberpunk rocker, neon guitar, futuristic city, glitch art"
        ),
        (
            "drum and bass, liquid dnb, fast breakbeats, deep sub bass, atmospheric pads, soulful, melodic", 
            "futuristic tunnel, speed lines, neon blue and orange, liquid fluid abstract"
        )
    ]
    return random.choice(genres)

# =========================
# 4. AUDIO GENERATION (STABLE AUDIO)
# =========================
def gen_music_stable_audio(prompt, out_wav, duration_sec=45):
    print(f"üéß StableAudio Generating: {prompt}...")
    sample_rate = 44100
    
    # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ —Ç—è–∂–µ–ª–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
    cleanup()
    
    with torch.no_grad():
        audio = generate_diffusion_cond(
            model=audio_model,
            conditioning=[{
                "prompt": prompt,
                "seconds_start": 0,
                "seconds_total": duration_sec
            }],
            steps=100,           # 100 —à–∞–≥–æ–≤ - –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏/–∫–∞—á–µ—Å—Ç–≤–∞ (–º–æ–∂–Ω–æ 150, –Ω–æ –±—É–¥–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–µ–µ)
            cfg_scale=7.0,       # 7.0 –¥–∞–µ—Ç —Ö–æ—Ä–æ—à–µ–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç—É
            sample_rate=sample_rate,
            device=DEVICE
        )

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    audio = audio / audio.abs().max().clamp(min=1e-6)
    audio = audio * 0.95 

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (–¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º —Ç–µ–Ω–∑–æ—Ä, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    # Stable Audio –≤—ã–¥–∞–µ—Ç [1, 2, samples], torchaudio –æ–∂–∏–¥–∞–µ—Ç [channels, samples] - –æ–±—ã—á–Ω–æ –æ–∫
    audio = audio.squeeze(0) # —É–±–∏—Ä–∞–µ–º batch dimension -> [2, samples]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ—Ä–µ–∑ soundfile –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ (torchaudio –∏–Ω–æ–≥–¥–∞ –∫–∞–ø—Ä–∏–∑–Ω–∏—á–∞–µ—Ç —Å —Ñ–æ—Ä–º–∞—Ç–∞–º–∏ –Ω–∞ RunPod)
    audio_np = audio.cpu().numpy().T # -> [samples, 2]
    sf.write(out_wav, audio_np, sample_rate)
    
    return sample_rate

# =========================
# 5. WORKER
# =========================
def generate_segment(idx, is_dj_turn):
    print(f"\nüî® [Worker] Processing segment {idx} (DJ Turn: {is_dj_turn})...")
    t0 = time.time()
    
    music_prompt, visual_prompt = get_vibes()
    
    # –§–∞–π–ª—ã
    music_path = os.path.join(WORKDIR, f"temp_music_{idx}.wav")
    voice_path = os.path.join(WORKDIR, f"temp_voice_{idx}.wav") if is_dj_turn else None
    cover_path = os.path.join(WORKDIR, f"temp_cover_{idx}.png")
    final_video = os.path.join(WORKDIR, f"segment_{idx}.ts")

    # A. Generate Music (Stable Audio)
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ~45-50 —Å–µ–∫—É–Ω–¥ (Stable Audio Open —É–º–µ–µ—Ç –¥–æ 47—Å)
    gen_music_stable_audio(music_prompt, music_path, duration_sec=45)

    # B. Generate Cover (SD)
    # –ß—Ç–æ–±—ã –Ω–µ –≤—ã–ª–µ—Ç–µ—Ç—å –ø–æ –ø–∞–º—è—Ç–∏, –º–æ–∂–Ω–æ –≤—ã–≥—Ä—É–∑–∏—Ç—å SD –≤ CPU, –µ—Å–ª–∏ –±—É–¥–µ—Ç OOM,
    # –ù–æ –Ω–∞ 16GB –¥–æ–ª–∂–Ω–æ –≤–ª–µ–∑—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ.
    with torch.no_grad():
        image = sd_pipe(f"{visual_prompt}, masterpiece, 8k", num_inference_steps=20).images[0]
    image.save(cover_path)

    # C. TTS
    if is_dj_turn:
        mood = music_prompt.split(",")[0]
        dj_text = ai_dj.generate_script(mood=mood)
        print(f"üó£Ô∏è DJ Says: {dj_text}")
        asyncio.run(edge_tts.Communicate(dj_text, "en-US-ChristopherNeural").save(voice_path))

    # D. Assembly
    f = sf.SoundFile(music_path)
    music_dur = len(f) / f.samplerate
    # Stable Audio –¥–µ–ª–∞–µ—Ç –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π —Ç—Ä–µ–∫, –µ–≥–æ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ª—É–ø–∏—Ç—å 3 —Ä–∞–∑–∞
    # –ù–æ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∑–∞–ª—É–ø–∏–º 2 —Ä–∞–∑–∞, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å ~1.5 –º–∏–Ω—É—Ç—ã —ç—Ñ–∏—Ä–∞
    total_dur = music_dur * 2 

    cmd = ["ffmpeg", "-y", "-loglevel", "error"]
    cmd += ["-loop", "1", "-i", cover_path]
    if is_dj_turn:
        cmd += ["-i", voice_path]
    
    cmd += ["-stream_loop", "1", "-i", music_path] # loop 1 —Ä–∞–∑ (–∏–≥—Ä–∞–µ—Ç 2 —Ä–∞–∑–∞)
    cmd += ["-t", str(total_dur)]

    if is_dj_turn:
        # Crossfade
        filter_str = "[1:a]volume=1.5[v];[2:a]volume=0.9[m];[v][m]amix=inputs=2:duration=longest:dropout_transition=2[mix];[mix]acompressor=ratio=4[aout]"
    else:
        filter_str = "[1:a]volume=1.0,acompressor=ratio=4[aout]"

    cmd += ["-filter_complex", filter_str]
    cmd += [
        "-map", "0:v", "-map", "[aout]",
        "-c:v", "libx264", "-preset", "fast", "-pix_fmt", "yuv420p", "-g", "60",
        "-c:a", "aac", "-b:a", "192k", "-ar", "44100",
        "-f", "mpegts", final_video
    ]
    
    subprocess.run(cmd, check=True)
    
    for f in [music_path, voice_path, cover_path]:
        if f and os.path.exists(f): os.remove(f)
    
    cleanup()
    print(f"‚úÖ [Worker] Segment {idx} ready ({round(time.time()-t0)}s)")
    return final_video

def worker_thread():
    idx = 0
    tracks_since_dj = 0
    while True:
        if video_queue.full():
            time.sleep(1)
            continue
        try:
            is_dj_turn = (tracks_since_dj >= TRACKS_BEFORE_DJ)
            seg_path = generate_segment(idx, is_dj_turn)
            video_queue.put(seg_path)
            idx += 1
            if is_dj_turn:
                tracks_since_dj = 0
            else:
                tracks_since_dj += 1
        except Exception as e:
            print(f"‚ùå Worker Error: {e}")
            time.sleep(5)

# =========================
# 6. STREAMER
# =========================
def streamer_thread():
    print("üì° Streamer started. Buffering...")
    # –ñ–¥–µ–º 1 –≥–æ—Ç–æ–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç. Stable Audio –º–µ–¥–ª–µ–Ω–Ω—ã–π, –ø–æ—ç—Ç–æ–º—É –±—É—Ñ–µ—Ä –Ω–∞–±–∏—Ä–∞–µ—Ç—Å—è –¥–æ–ª—å—à–µ.
    while video_queue.qsize() < 1:
        time.sleep(5)
    print("üî¥ GOING LIVE!")

    stream_cmd = [
        "ffmpeg", "-re",
        "-f", "mpegts", "-i", "pipe:0",
        "-c", "copy",
        "-f", "flv", RTMP_URL
    ]
    
    process = subprocess.Popen(stream_cmd, stdin=subprocess.PIPE)

    while True:
        seg_path = video_queue.get()
        print(f"‚ñ∂Ô∏è Playing: {seg_path} (Queue: {video_queue.qsize()})")
        
        try:
            with open(seg_path, "rb") as f:
                while True:
                    chunk = f.read(4096 * 10)
                    if not chunk: break
                    process.stdin.write(chunk)
            process.stdin.flush()
        except BrokenPipeError:
            print("‚ùå Stream pipe broken. Restarting FFmpeg...")
            process = subprocess.Popen(stream_cmd, stdin=subprocess.PIPE)
        except Exception as e:
            print(f"‚ùå Streamer Error: {e}")

        if os.path.exists(seg_path):
            os.remove(seg_path)

if __name__ == "__main__":
    t_worker = threading.Thread(target=worker_thread, daemon=True)
    t_worker.start()
    streamer_thread()
